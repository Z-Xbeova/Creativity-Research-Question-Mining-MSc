{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "135f9d96-b383-4458-8553-f206a8fced04",
   "metadata": {},
   "source": [
    "## Prepare data and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d8ad598-585a-45e1-b359-4a680c9e2806",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kmarc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\utils\\hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TRANSFORMERS_CACHE'] = 'transformers_cache'\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktParameters\n",
    "from transformers import BertTokenizer, BertModel, BertConfig\n",
    "import string\n",
    "import time\n",
    "import torch\n",
    "from textstat import sentence_count\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bbe33d8-1ff8-4905-ad24-52a3cac8e230",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = ['all-distilroberta-v1', 'all-MiniLM-L12-v2', 'all-mpnet-base-v2']\n",
    "\n",
    "filepath = 'data/sta/e4_content/int{number}_bloom.csv'\n",
    "filepath_inf = 'data/sta/e4_content/int{number}_bloom_inf.csv'\n",
    "filepath_dsi = 'data/sta/e4_content/int{number}_bloom_dsi.csv'\n",
    "filepath_mad = 'data/sta/e4_content/int{number}_bloom_mad.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc2ed514-455d-49ea-9530-accdb0b20ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_ds = lambda number: pd.read_csv(filepath.format(number=number), sep=',', index_col=0)\n",
    "load_model = lambda number: SentenceTransformer(model_names[number])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "763e4645-6ffd-4fa5-add8-7e4cd81a2e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_metric(string):\n",
    "    string_list = string.split()\n",
    "    counts = np.unique(string_list, return_counts=True)[1]\n",
    "    numerator = np.sum(counts*(counts-1))\n",
    "    n = len(string_list)\n",
    "    denominator = n*(n-1)\n",
    "    return numerator/denominator\n",
    "\n",
    "topwords = list(pd.read_csv('1-1000.txt', sep='~', header=None)[0])\n",
    "def common_words(string):\n",
    "    string_list = string.split()\n",
    "    result = sum([word in topwords for word in string_list])\n",
    "    return result\n",
    "\n",
    "def sentence_analysis(ds):\n",
    "    ds['CharacterNumber'] = ds.Question.str.len()\n",
    "    ds['WordNumber'] = ds.Question.str.count(' ') + 1\n",
    "    ds['CommonWordNumber'] = ds.Question.apply(common_words) / ds.WordNumber\n",
    "    ds['UniqueWordNumber'] = ds.Question.str.split().apply(set).apply(len)\n",
    "    ds['TTR'] = ds.UniqueWordNumber / ds.WordNumber\n",
    "    ds['CTTR'] = ds.UniqueWordNumber / (ds.WordNumber*2)**0.5\n",
    "    ds['DMetric'] = ds.Question.apply(d_metric)\n",
    "    ds['SyllableNumber'] = ds.Question.str.count('(?!e$)[aeiouy]+') + ds.Question.str.count('^[^aeiouy]*e$')\n",
    "    ds['SentenceNumber'] = ds.Question.apply(sentence_count)\n",
    "    ds['MeanSentenceLength'] = ds.CharacterNumber/ds.SentenceNumber\n",
    "    ds['FRES'] = 206.835 - 1.015 * ds.MeanSentenceLength - 84.6 * ds.SyllableNumber / ds.WordNumber\n",
    "    ds['FKGL'] = 0.39 * ds.MeanSentenceLength + 11.8 * ds.SyllableNumber / ds.WordNumber - 15.59\n",
    "    ds['ARI'] = 0.5 * ds.MeanSentenceLength + 47.1 * ds.CharacterNumber / ds.WordNumber - 21.34\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7f91110-7f31-4da4-9c56-c66289a2bfaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name sentence-transformers/all-distilroberta-v1. Creating a new one with MEAN pooling.\n",
      "C:\\Users\\kmarc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "No sentence-transformers model found with name sentence-transformers/all-MiniLM-L12-v2. Creating a new one with MEAN pooling.\n",
      "No sentence-transformers model found with name sentence-transformers/all-mpnet-base-v2. Creating a new one with MEAN pooling.\n"
     ]
    }
   ],
   "source": [
    "droberta = load_model(0)\n",
    "minilm = load_model(1)\n",
    "mpnet = load_model(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10b69d4-52fe-452f-a3d4-07f61684b3d9",
   "metadata": {},
   "source": [
    "# Prepare metrics from Answer-Based Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c739c0ba-5faf-4daa-acef-9a30f8d221a3",
   "metadata": {},
   "source": [
    "## Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87b1e865-1cff-48ec-ace4-0ff7a3ddd2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def information_metric(dataset):\n",
    "    dataset = sentence_analysis(dataset)\n",
    "    dataset['QA'] = dataset.Question + ' ' + dataset.Answer\n",
    "    for index, stmodel in enumerate([droberta, minilm, mpnet]):\n",
    "        question_embeds = stmodel.encode(dataset.Question)\n",
    "        qa_embeds = stmodel.encode(dataset.QA)\n",
    "        dataset['distances_'+str(index)] = np.diag(cosine_similarity(question_embeds, qa_embeds))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bece98a-d02c-4b90-8c90-bfb4e7657a67",
   "metadata": {},
   "source": [
    "## DSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03448525-064c-41eb-b824-efe2a306a20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertModel.from_pretrained(\"bert-large-uncased\", output_hidden_states = True) # initialize BERT model instance\n",
    "model.eval()\n",
    "segmenter = PunktSentenceTokenizer() # initialize segmenter: does sentence segmentation, returns list\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased') # initialize BERT tokenizer\n",
    "cos = torch.nn.CosineSimilarity(dim = 0)\n",
    "\n",
    "filter_list = np.array(['[CLS]', '[PAD]', '[SEP]', '.', ',', '!', '?'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2bce742c-3164-4229-b58f-cc22df6c16cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dsi(dataset, verbose=False):\n",
    "    s = {}\n",
    "    # SEGMENT DATA INTO SENTENCES\n",
    "    start_time = time.time()\n",
    "    for ID, row in tqdm(dataset.iterrows()):\n",
    "        try:\n",
    "            # ID = index  # get current participant ID\n",
    "            text = row[\"Answer\"]  # get current story\n",
    "            s[ID] = {}  # add dict entry for subject and create nested dict to store subject data\n",
    "        \n",
    "            # TRAIN SENTENCE SEGEMENTER AND SEGMENT SENTENCE\n",
    "            segmenter.train(text) # train the segmenter on the text first (unsupervised algorithm that is pretrained and can improve with added training)\n",
    "            sentences_from_segmenter = segmenter.tokenize(text) # apply the additionally-trained segmenter to the text\n",
    "        \n",
    "            # LOOP OVER SENTENCES AND GET BERT FEATURES (LAYERS 6 & 7)\n",
    "            features = []  # initialize list to store dcos values, one for each sentence\n",
    "            words = []\n",
    "            for i in range(len(sentences_from_segmenter)):  # loop over sentences\n",
    "                sentence = sentences_from_segmenter[i].translate(str.maketrans('', '', string.punctuation))\n",
    "                sent_tokens = tokenizer(sentence, max_length = 50, truncation = True, padding = 'max_length', return_tensors=\"pt\")\n",
    "                sent_words = [tokenizer.decode([k]) for k in sent_tokens['input_ids'][0]]\n",
    "                sent_indices = np.where(np.in1d(sent_words, filter_list, invert = True))[0]  # we'll use this to filter out special tokens and punctuation\n",
    "                with torch.no_grad():\n",
    "                    sent_output = model(**sent_tokens)# feed model the sentence tokens and get outputs\n",
    "                    hids = sent_output.hidden_states # isolate hidden layer activations\n",
    "                layer6 = hids[6] # isolate layer 6 hidden activations\n",
    "                layer7 = hids[7] # do the same for layer 7\n",
    "        \n",
    "                for j in sent_indices:  # loop over words and create list of all hidden vectors from layers 6 & 7; additionally store number of words (doubled, to account for layer 6 and 7 duplicates)\n",
    "                    words.append(sent_words[j])\n",
    "                    words.append(sent_words[j])\n",
    "                    features.append(layer6[0,j,:])  # layer 6 features\n",
    "                    features.append(layer7[0,j,:])  # layer 7 features\n",
    "        \n",
    "            # GET DCOS VALUES FOR STORY\n",
    "            num_words = len(words) # number of words, in terms of hidden activation vectors (2*words)\n",
    "            lower_triangle_indices = np.tril_indices_from(np.random.rand(num_words, num_words), k = -1)  # creates a matrix that represents words*2 (i.e., from word representations from both layer 6+7) and gets the indices of the lower triangle, omitting diagonal (k = -1)A\n",
    "            story_dcos_vals = []  # intialize storage for dcos of current sentence\n",
    "            for k in range(len(lower_triangle_indices[0])): # loop over lower triangle indices\n",
    "                features1 = features[lower_triangle_indices[0][k]]\n",
    "                features2 = features[lower_triangle_indices[1][k]]\n",
    "                dcos = (1-cos(features1, features2))  # compute dcos\n",
    "                story_dcos_vals.append(dcos) # store dcos value in list\n",
    "        \n",
    "            mean_story_dcos = torch.mean(torch.stack(story_dcos_vals)).item()  # get average story dcos\n",
    "            s[ID][\"DSI\"] = mean_story_dcos\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    # MERGE OUTPUT WITH INPUT DATAFRAME\n",
    "    dsi_df = pd.DataFrame.from_dict(s, orient = \"index\") # make pandas dataframe from DSI dictionary\n",
    "    # dsi_df[\"ID\"] = dsi_df.index\n",
    "    # dsi_df.to_csv('DSI_output.csv', index = False) # save updated dataframe\n",
    "    if verbose:\n",
    "        elapsed_time = time.time()-start_time # get elapsed time to compute DSI values\n",
    "        print('elapsed time: ' + str(elapsed_time)) # display elapsed time (in seconds)\n",
    "    return dsi_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8870a51-a399-4fb0-9d88-e224845d54bd",
   "metadata": {},
   "source": [
    "# Compute metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5361e87f-cc89-4829-84d5-668339f822fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14it [02:33, 10.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [00:39,  3.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [18:09, 136.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [03:54, 21.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [01:14, 14.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16it [03:05, 11.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16it [14:24, 54.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:22,  5.72s/it]\n"
     ]
    }
   ],
   "source": [
    "for number in range(1, 9):\n",
    "    print(number)\n",
    "    ds = load_ds(number).reset_index()\n",
    "    ds_inf = information_metric(ds)\n",
    "    ds_inf.to_csv(filepath_inf.format(number=number), sep=',')\n",
    "    ds_dsi = dsi(ds)\n",
    "    ds_dsi.to_csv(filepath_dsi.format(number=number), sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "537a67d3-b4ac-42b3-9b25-834bc2483a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "for number in range(1, 9):\n",
    "    print(number)\n",
    "    ds = load_ds(number).reset_index()\n",
    "    ds_mad = mad_dataset(ds)\n",
    "    ds_mad.to_csv(filepath_mad.format(number=number), sep=';')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
